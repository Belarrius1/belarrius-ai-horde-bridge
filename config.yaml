# Belarrius AI Horde Bridge v1.2
# Start command: node index.js

# AI Horde API key (required)
AiHordeApiKey: ""

# Worker identity shown on Horde
workerName: "Worker"

# Horde cluster URL
clusterUrl: "https://stablehorde.net"

# Optional: usernames to prioritize
# Format:
# priorityUsernames:
#   - "Belarrius#229816"
# Keep [] to disable.
priorityUsernames: []

# Optional: update worker description on startup via AI Horde API.
# This requires your worker id.
# If either value is empty, no PUT is sent.
workerId: ""
workerInfo: ""

# Local inference engine (required)
# Possible values: ollama, oobabooga, llamacpp, koboldcpp, vllm, sglang, tabbyapi
# Aliases accepted for compatibility: textgenwebui, oogabooga, lmstudio, localai, mistralrs, mistral.rs, mlx, openllm, aphrodite, aphrodite-engine
# LM Studio, LocalAI, mistral.rs and many OpenAI-compatible servers should work via this mode.
serverEngine: "llamacpp"

# Local server URL
# For ollama default is usually: http://localhost:11434
# For oobabooga/LM Studio/LocalAI OpenAI endpoint:
# usually http://localhost:5000 (WebUI), http://localhost:1234 (LM Studio), http://localhost:8080 (LocalAI)
# mistral.rs and MLX ports depend on your server command/options.
serverUrl: "http://localhost:8000"

# Optional: Authorization header sent to local server ("" = disabled)
# For oobabooga, you can set either:
# - raw key (bridge will send "Bearer <key>")
# - full header value (example: "Bearer <key>")
serverApiKey: ""

# Optional: model override sent to local server ("" = disabled)
# Required when serverEngine is "ollama" (example: llama3.1:8b)
# Required when serverEngine is "oobabooga", "lmstudio", "localai", "mistralrs"/"mistral.rs", "mlx", "openllm" or "aphrodite"
serverModel: ""

# Model name advertised to AI Horde (required)
# For ollama you can use: ollama/your-model-name
# For oobabooga you can use: oobabooga/your-model-name
# For LM Studio you can use: lmstudio/your-model-name
# For LocalAI you can use: localai/your-model-name
# For mistral.rs you can use: mistralrs/your-model-name
# For MLX you can use: mlx/your-model-name
# For OpenLLM you can use: openllm/your-model-name
# For Aphrodite you can use: aphrodite/your-model-name
model: "llamacpp/your-model-name"

# Context & generation settings
# Context size (required)
ctx: 4096
maxLength: 512
# Possible values: enabled, disabled
# If enabled, the bridge rejects jobs when estimated prompt tokens exceed:
# ctx - requested max_length
enforceCtxLimit: "disabled"

# Number of parallel jobs processed by this bridge.
# 1 = safest/stable default.
# Increase only if your backend can run multiple generations in parallel.
threads: 1

# Polling stagger between threads.
# Possible values: enabled, disabled
# Useful with 4+ threads to smooth Horde API traffic and avoid burst polling.
# Example with threads=4 and refreshTime=5000:
# each thread polls with a ~1250ms phase offset.
threadPollStagger: "enabled"

# AI Horde Network settings
# timeout (seconds): max wait for Horde pop/submit and local generation HTTP calls.
# Use higher values for long prompts or slower hardware.
timeout: 120

# refreshTime (milliseconds): poll interval for new Horde jobs.
# Lower = faster pickup but more API traffic.
# Higher = less traffic but more idle latency between jobs.
refreshTime: 5000

# Safety
# Possible values: enabled, disabled
nsfw: "disabled"

# CSAM filter mode:
# - disabled: no bridge-side CSAM filtering
# - regex: local keyword/regex checks
# - openai: OpenAI omni-moderation-latest (requires openaiApiKey)
#   If OpenAI moderation is unavailable, jobs are faulted (fail-closed).
#   Decision is based on OpenAI category boolean "sexual/minors" only.
enableCsamFilter: "regex"

# Action when a positive CSAM detection occurs (regex/openai):
# - respond: submit a safe explanatory text instead of faulting the job (recommended)
# - fault: keep old behavior (can increase worker drops/maintenance risk)
csamPositiveAction: "respond"

# Text sent to requester when csamPositiveAction is "respond"
csamBlockedResponse: "Your request has been filtered by this worker safety policy."

# Optional metadata ref for Horde gen_metadata when CSAM is responded.
# Leave empty for auto value.
csamMetadataRef: ""

# OpenAI moderation input token cap per prompt.
# If estimated prompt tokens exceed this value, the prompt is truncated (head only)
# before calling OpenAI moderation.
# Range: 1..30000
# Note: values above 30000 are not useful for omni-moderation-latest.
openaiModerationMaxTokens: 10000

# Required only when enableCsamFilter is "openai"
openaiApiKey: ""

# Throughput limit (tokens/sec), "" to disable
maxTps: ""

# UI theme
# Possible values: acide, cyberpunk, matrix, monochrome
uiTheme: "acide"

# Logs disabled by default (empty string = disabled)
outputPrompt: ""
logFile: ""
